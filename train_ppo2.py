import argparse
import os
import time
import gc
import random
from collections import defaultdict

import torch
from torch.optim import AdamW
import numpy as np
from tqdm import tqdm

from datasets import load_dataset, Dataset, concatenate_datasets

from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    AutoModelForCausalLM,
    DataCollatorWithPadding,
)

from trl import (
    PPOTrainer,
    PPOConfig,
    create_reference_model,
    AutoModelForCausalLMWithValueHead,
)

from reward_utils import set_seed, get_raw_preference_scores, get_toxicity_scores

# ---------------- CONSTANTS ----------------
SPECIAL_TOKENS = {"user": "<|user|>", "bot": "<|bot|>", "end": "<|endofbot|>"}
PREF_MEAN = 0.3904
PREF_STD = 1.8893
TOX_MEAN = 0.4729 
TOX_STD = 0.5

EPSILON = 1e-8 # For safe division

MAX_DAILYDIALOG_PROMPTS = 7000
MAX_OASST1_PROMPTS = 3000
MAX_HH_PROMPTS = 2500
MAX_TOXICITY_PROMPTS = 2500

W_PREFERENCE = 1.0 
W_TOXICITY = 1.0   

INVALID_RESPONSE_REWARD = -2.0

# ---------------- Helper Functions ----------------
def format_prompt(text, tokenizer_to_use):
    messages = [{"role": "user", "content": text}]
    return tokenizer_to_use.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)

def safe_to_string(x):
    if isinstance(x, bytes):
        return x.decode("utf-8", errors="ignore")
    elif isinstance(x, torch.Tensor):
        try:
             return str(x.tolist()) 
        except:
             return str(x)
    elif isinstance(x, (list, dict)):
        return str(x)
    else:
        return str(x)


def process_daily_dialog_prompts(tokenizer_, limit=MAX_DAILYDIALOG_PROMPTS):
    print(f"Loading DailyDialog prompts (limit {limit})...")
    try:
        raw = load_dataset("daily_dialog", split="train", cache_dir=args.cache_dir)
    except Exception as e:
        print(f"Could not load DailyDialog: {e}. Skipping.")
        return None
    prompts = []
    processed_ids = set()
    raw = raw.shuffle(seed=42)
    for ex in raw:
        if len(prompts) >= limit: break
        dialog_id = ex.get("id", hash(tuple(ex["dialog"])))
        if dialog_id in processed_ids: continue
        user_turns = ex["dialog"][::2]
        if user_turns:
            text = safe_to_string(user_turns[0])
            prompts.append({"prompt": format_prompt(text, tokenizer_)})
            processed_ids.add(dialog_id)
    print(f"Loaded {len(prompts)} prompts from DailyDialog.")
    return Dataset.from_list(prompts) if prompts else None

def process_oasst1_prompts(tokenizer_, limit=MAX_OASST1_PROMPTS):
    print(f"Loading OASST1 prompts (limit {limit})...")
    try:
        ds = load_dataset("OpenAssistant/oasst1", split="train", cache_dir=args.cache_dir)
    except Exception as e:
        print(f"Could not load OASST1: {e}. Skipping.")
        return None
    ds_filtered = ds.filter(lambda x: x.get("lang", "") == "en" and x.get("parent_id") is None and x.get("role") == "prompter")
    if len(ds_filtered) > limit:
        ds_filtered = ds_filtered.shuffle(seed=42).select(range(limit))
    prompts = [{"prompt": format_prompt(safe_to_string(example["text"]), tokenizer_)} for example in ds_filtered] # Use safe_to_string
    print(f"Loaded {len(prompts)} prompts from OASST1.")
    return Dataset.from_list(prompts) if prompts else None

def process_anthropic_hh_prompts(tokenizer_, limit=MAX_HH_PROMPTS):
    print(f"Loading Anthropic HH prompts (limit {limit})...")
    prompts = []
    loaded = 0
    try:
        for subset in ["harmless-base", "helpful-base"]:
            if loaded >= limit: break
            ds = load_dataset("Anthropic/hh-rlhf", data_dir=subset, split="train", cache_dir=args.cache_dir)
            ds = ds.shuffle(seed=42)
            for ex in ds:
                if loaded >= limit: break
                conversation = ex.get("chosen", "")
                human_turns = conversation.split("\n\nHuman:")
                if len(human_turns) > 1:
                    first_prompt_text = human_turns[1].split("\n\nAssistant:")[0].strip()
                    if first_prompt_text:
                        prompts.append({"prompt": format_prompt(safe_to_string(first_prompt_text), tokenizer_)}) # Use safe_to_string
                        loaded += 1
    except Exception as e:
        print(f"Could not load Anthropic/hh-rlhf: {e}")
    print(f"Loaded {len(prompts)} prompts from Anthropic HH.")
    return Dataset.from_list(prompts) if prompts else None

def process_real_toxicity_prompts(tokenizer_, limit=MAX_TOXICITY_PROMPTS):
    print(f"Loading RealToxicityPrompts (limit {limit})...")
    try:
        ds = load_dataset("allenai/real-toxicity-prompts", split="train", cache_dir=args.cache_dir)
        ds = ds.shuffle(seed=42).select(range(min(limit, len(ds))))
    except Exception as e:
        print(f"Could not load RealToxicityPrompts: {e}. Skipping.")
        return None
    prompts = [{"prompt": format_prompt(safe_to_string(example["prompt"]["text"]), tokenizer_)} for example in ds] # Use safe_to_string
    print(f"Loaded {len(prompts)} prompts from RealToxicityPrompts.")
    return Dataset.from_list(prompts) if prompts else None

def get_prompt_dataset(tokenizer_):
    datasets_to_combine = []
    for fn in [process_daily_dialog_prompts,
        process_oasst1_prompts,
        process_anthropic_hh_prompts,
        process_real_toxicity_prompts]:
        d = fn(tokenizer_)
        if d is not None:
            datasets_to_combine.append(d)
    if not datasets_to_combine:
        raise ValueError("No prompt data loaded!")

    combined = concatenate_datasets(datasets_to_combine).shuffle(seed=42)
    print(f"Total combined prompts before tokenization: {len(combined)}")

    def tokenize_prompt(element):
        text = safe_to_string(element["prompt"]) 
        tokenized = tokenizer_(
            text,
            truncation=True,
            max_length=args.input_max_len,
            padding="max_length",
            return_tensors="pt",
        )
        return {
            "input_ids": tokenized["input_ids"].squeeze(0),
            "attention_mask": tokenized["attention_mask"].squeeze(0),
            "query": text,
        }

    combined = combined.map(tokenize_prompt, remove_columns=["prompt"], batched=False) # Use batched=False for simplicity

    combined.set_format(type="torch", columns=["input_ids", "attention_mask"])
    print(f"Total prompts for PPO: {len(combined)}")
    return combined

def get_combined_rewards(
    prompts_text, responses_text,
    pref_model, pref_tokenizer,
    tox_model, tox_tokenizer,
    device, args
):
    """Calculates combined, NORMALIZED reward. Returns list of floats."""
    if not prompts_text or not responses_text:
        return []

    # 1. Normalized Preference Score
    pref_inputs = [p + r for p, r in zip(prompts_text, responses_text)]
    raw_pref_scores = get_raw_preference_scores(
        pref_model, pref_tokenizer, pref_inputs,
        args.reward_batch_size, device, args.pref_max_len
    )
    raw_pref_scores = np.array(raw_pref_scores)
    norm_pref_scores = (raw_pref_scores - PREF_MEAN) / (PREF_STD + EPSILON)

    # 2. Normalized Toxicity (Safety) Score
    raw_safety_scores = get_toxicity_scores(
        tox_model, tox_tokenizer, responses_text,
        args.reward_batch_size, device, args.tox_max_len
    )
    raw_safety_scores = np.array(raw_safety_scores)
    norm_safety_scores = (raw_safety_scores - TOX_MEAN) / (TOX_STD + EPSILON)

    # 3. Combine NORMALIZED scores
    final_scores = []
    for pref_score, safety_score in zip(norm_pref_scores, norm_safety_scores):
        score = (W_PREFERENCE * float(pref_score)) + (W_TOXICITY * float(safety_score))
        score = np.clip(score, -args.clip_reward, args.clip_reward)
        final_scores.append(float(score)) # Ensure float

    return final_scores


# ---------------- Main ----------------
if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--sft_model_dir", type=str, required=True)
    parser.add_argument("--tox_model_dir", type=str, required=True)
    parser.add_argument("--pref_model_dir", type=str, required=True)
    parser.add_argument("--output_dir", type=str, default="ppo-output-stable")
    parser.add_argument("--cache_dir", type=str, default=None)
    parser.add_argument("--input_max_len", type=int, default=128)
    parser.add_argument("--output_max_len", type=int, default=64)
    parser.add_argument("--tox_max_len", type=int, default=128)
    parser.add_argument("--pref_max_len", type=int, default=512)
    parser.add_argument("--reward_batch_size", type=int, default=16)
    parser.add_argument("--ppo_epochs", type=int, default=1) # Fewer epochs can be more stable
    parser.add_argument("--learning_rate", type=float, default=1e-6) # Keep small
    parser.add_argument("--batch_size", type=int, default=16)
    parser.add_argument("--mini_batch_size", type=int, default=4)
    parser.add_argument("--gradient_accumulation_steps", type=int, default=1)
    parser.add_argument("--adap_kl_ctrl", type=bool, default=True) # Keep True
    # --- âœ¨ Set a reasonable default for init_kl_coef ---
    parser.add_argument("--init_kl_coef", type=float, default=0.05) # Lower starting value
    # ----------------------------------------------------
    parser.add_argument("--target_kl", type=float, default=0.1) # Slightly lower target
    parser.add_argument("--clip_reward", type=float, default=5.0) # Lower clip range post-norm
    parser.add_argument("--seed", type=int, default=42)
    parser.add_argument("--save_every_n_steps", type=int, default=0)

    parser.add_argument("--vf_coef", type=float, default=0.5) # Increased slightly
    parser.add_argument("--max_grad_norm", type=float, default=0.5)
    args = parser.parse_args()

    set_seed(args.seed)
    if not torch.cuda.is_available():
        raise SystemExit("CUDA required.")
    device = torch.device("cuda")
    print(f"Using {torch.cuda.get_device_name(0)}")

    tokenizer = AutoTokenizer.from_pretrained(args.sft_model_dir, cache_dir=args.cache_dir, padding_side="left")
    tokenizer.pad_token = tokenizer.eos_token
    tox_tokenizer = AutoTokenizer.from_pretrained(args.tox_model_dir, cache_dir=args.cache_dir)
    pref_tokenizer = AutoTokenizer.from_pretrained(args.pref_model_dir, cache_dir=args.cache_dir)
    if tox_tokenizer.pad_token is None: tox_tokenizer.pad_token = tox_tokenizer.eos_token
    if pref_tokenizer.pad_token is None: pref_tokenizer.pad_token = pref_tokenizer.eos_token

    model = AutoModelForCausalLMWithValueHead.from_pretrained(args.sft_model_dir, cache_dir=args.cache_dir,torch_dtype=torch.float32).to(device)


    print("Zero-initializing value head for stability...")
    with torch.no_grad():
        for param in model.v_head.parameters():
            param.zero_()

    ref_model = create_reference_model(model).to(device)
    ref_model = ref_model.to(torch.float32)
    ref_model.eval()
    tox_model = AutoModelForSequenceClassification.from_pretrained(args.tox_model_dir, cache_dir=args.cache_dir).to(device)
    pref_model = AutoModelForSequenceClassification.from_pretrained(args.pref_model_dir, cache_dir=args.cache_dir).to(device)
    tox_model.eval()
    pref_model.eval()

    dataset = get_prompt_dataset(tokenizer)
    data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors="pt")

    ppo_config = PPOConfig(
        model_name=args.sft_model_dir,
        learning_rate=args.learning_rate,
        batch_size=args.batch_size,
        mini_batch_size=args.mini_batch_size,
        gradient_accumulation_steps=args.gradient_accumulation_steps,
        ppo_epochs=args.ppo_epochs,
        kl_penalty="kl",
        adap_kl_ctrl=args.adap_kl_ctrl,
        init_kl_coef=args.init_kl_coef, 
        target=args.target_kl,
        cliprange=0.1, # Use default or add as arg if needed
        cliprange_value=0.2,
        log_with="tensorboard",
        accelerator_kwargs={"mixed_precision": "no"},
        project_kwargs={"logging_dir": os.path.join(args.output_dir, "logs")},
        use_score_norm=False, # TRL handles batch normalization
        score_clip=None, # TRL handles clipping after normalization
        seed=args.seed,
        optimize_cuda_cache=True,
        vf_coef=args.vf_coef,           # <-- Added/Updated
        max_grad_norm=args.max_grad_norm
    )
    # -----------------------------

    ppo_trainer = PPOTrainer(
        config=ppo_config,
        model=model,
        ref_model=ref_model,
        tokenizer=tokenizer,
        dataset=dataset,
        data_collator=data_collator # Use the collator
    )

    end_token_id = tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS["end"])

    # --- Generation kwargs ---
    generation_kwargs = dict(
        min_length=-1,
        top_k=40, 
        top_p=0.9, 
        do_sample=True,
        pad_token_id=end_token_id,
        eos_token_id=end_token_id,
        max_new_tokens=args.output_max_len,
        temperature=0.7, 
    )

    print("Starting PPO training...")
    # Calculate total steps correctly based on epochs
    total_steps_per_epoch = len(ppo_trainer.dataloader)
    total_ppo_steps = total_steps_per_epoch * ppo_config.ppo_epochs
    print(f"Total steps: {total_ppo_steps} ({total_steps_per_epoch} per epoch)")

    global_step = 0

    # --- Explicit Epoch Loop ---
    for epoch in range(ppo_config.ppo_epochs):
        print(f"--- Epoch {epoch+1}/{ppo_config.ppo_epochs} ---")
        for step, batch in tqdm(enumerate(ppo_trainer.dataloader), total=total_steps_per_epoch, desc=f"Epoch {epoch+1}"):
            # Data collator provides tensors
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            query_texts = batch.get("query", [""] * input_ids.shape[0]) # Get strings
            bs = input_ids.shape[0]

            # Prepare list of tensors for generate
            query_tensors_list = [input_ids[i].detach() for i in range(bs)]

            start_time = time.time()
            try:
                response_tensors = ppo_trainer.generate(
                    query_tensors_list,
                    return_prompt=False,
                    **generation_kwargs,
                ) 
            except Exception as e:
                print(f"Generation failed at step {step}: {e}")
                gc.collect(); torch.cuda.empty_cache(); continue
            gen_time = time.time() - start_time

            try:
                response_texts = tokenizer.batch_decode(response_tensors, skip_special_tokens=True)
            except Exception as e:
                print(f"Error decoding responses at step {step}: {e}"); continue

            # --- Compute rewards ---
            reward_start_time = time.time()
            valid_idx = [i for i, r in enumerate(response_texts) if r and r.strip()]
            # Create rewards tensor on the correct device
            rewards_tensor = torch.full((bs,), INVALID_RESPONSE_REWARD, dtype=torch.float32, device=ppo_trainer.accelerator.device)

            if valid_idx:
                valid_prompts = [query_texts[i] for i in valid_idx]
                valid_responses = [response_texts[i] for i in valid_idx]
                combined_scores = get_combined_rewards(
                    valid_prompts, valid_responses,
                    pref_model, pref_tokenizer, tox_model, tox_tokenizer,
                    device, args # Use main device for RM inference
                )
                if len(combined_scores) == len(valid_idx):
                    scores_tensor = torch.tensor(combined_scores, dtype=torch.float32, device=rewards_tensor.device)
                    rewards_tensor[torch.tensor(valid_idx, device=rewards_tensor.device)] = scores_tensor
                else:
                    print(f"Warning: Reward/valid mismatch step {step}. Skipping.")

            reward_time = time.time() - reward_start_time

            try:
                rewards_mean = rewards_tensor.mean().item()
                rewards_std = rewards_tensor.std().item()
                rewards_min = rewards_tensor.min().item()
                rewards_max = rewards_tensor.max().item()
                print(f"\n[Step {global_step} Epoch {epoch+1}] Rewards Stats (before TRL norm): "
                    f"Mean={rewards_mean:.4f}, Std={rewards_std:.4f}, "
                    f"Min={rewards_min:.4f}, Max={rewards_max:.4f}")
                # Optional: Log shapes to double-check
                print(f"  Query List Len: {len(query_tensors_list)}, Resp List Len: {len(response_tensors)}")
                print(f"  Rewards Tensor Shape: {rewards_tensor.shape}")
            except Exception as log_e:
                print(f"Error during pre-step logging: {log_e}")

           
            rewards_list_of_tensors = [r for r in rewards_tensor]

            # Perform PPO step
            try:
                stats = ppo_trainer.step(
                    query_tensors_list,  # Already a List[Tensor] on device
                    response_tensors,    # Already a List[Tensor] on device
                    rewards_list_of_tensors
                )

                # --- âœ¨ Safely Get and Format Stats âœ¨ ---
                vf_loss = stats.get('ppo/loss/value', None)
                policy_loss = stats.get('ppo/loss/policy', None)
                kl_div = stats.get('objective/kl', None)

                # Format safely, printing 'N/A' if value is None
                kl_str = f"{kl_div:.4f}" if kl_div is not None else "N/A"
                vf_loss_str = f"{vf_loss:.4f}" if vf_loss is not None else "N/A"
                policy_loss_str = f"{policy_loss:.4f}" if policy_loss is not None else "N/A"

                print(f"  PPO Step Output: KL={kl_str}, VF Loss={vf_loss_str}, Policy Loss={policy_loss_str}")
                # ----------------------------------------

                # Add custom stats AFTER they are computed/retrieved
                stats["custom/gen_time"] = gen_time
                stats["custom/reward_time"] = reward_time
                stats["custom/non_empty_responses"] = len(valid_idx) / bs if bs > 0 else 0.0

                # Log stats using TRL's method (handles None implicitly for logging)
                ppo_trainer.log_stats(stats, {"query": query_texts[:3]}, rewards_tensor.cpu().tolist())

            except Exception as e:
                print(f"Error during PPO step {step} in epoch {epoch+1}: {e}")
                gc.collect(); torch.cuda.empty_cache(); continue

            # Housekeeping
            global_step += 1
            if global_step % 50 == 0:
                gc.collect(); torch.cuda.empty_cache()

            # Optional periodic save
            if args.save_every_n_steps > 0 and (global_step % args.save_every_n_steps == 0):
                print(f"Saving intermediate model at step {global_step}...")
                save_dir = os.path.join(args.output_dir, f"checkpoint-{global_step}")
                os.makedirs(save_dir, exist_ok=True) # Ensure dir exists
                ppo_trainer.save_pretrained(save_dir)
                tokenizer.save_pretrained(save_dir)

    print("PPO Training complete. Saving final model...")
    os.makedirs(args.output_dir, exist_ok=True)
    ppo_trainer.save_pretrained(args.output_dir)
    tokenizer.save_pretrained(args.output_dir)
    print("Saved to:", args.output_dir)