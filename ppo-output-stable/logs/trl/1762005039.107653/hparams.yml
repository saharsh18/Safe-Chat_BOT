accelerator_kwargs/mixed_precision: 'no'
adap_kl_ctrl: true
backward_batch_size: 4
batch_size: 16
cliprange: 0.1
cliprange_value: 0.2
compare_steps: 1
early_stopping: false
exp_name: train_ppo2
forward_batch_size: null
gamma: 1
global_backward_batch_size: 4
global_batch_size: 16
gradient_accumulation_steps: 1
gradient_checkpointing: false
horizon: 10000
init_kl_coef: 0.1
is_encoder_decoder: false
is_peft_model: false
kl_penalty: kl
lam: 0.95
learning_rate: 1.0e-06
log_with: tensorboard
max_grad_norm: 0.5
mini_batch_size: 4
model_name: gpt2-medium-chatbot-finetuned
optimize_cuda_cache: true
optimize_device_cache: false
ppo_epochs: 1
project_kwargs/logging_dir: ppo-output-stable/logs
query_dataset: imdb
ratio_threshold: 10.0
remove_unused_columns: true
reward_model: sentiment-analysis:lvwerra/distilbert-imdb
score_clip: null
seed: 42
steps: 20000
target: 0.1
target_kl: 1
task_name: null
total_ppo_epochs: 1250
tracker_project_name: trl
use_score_norm: false
use_score_scaling: false
vf_coef: 0.5
whiten_rewards: false
world_size: 1
